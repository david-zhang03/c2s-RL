{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817f06da-337c-4889-9bc2-f5882aeda826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sz568/.conda/envs/C2S_rl310/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets,load_from_disk\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbcf19d-3fc2-4985-b69a-220a3ca682d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk('hf_dataset_2024-12-06')\n",
    "ds\n",
    "# ds = ds.train_test_split(test_size=0.1)\n",
    "test_set = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb04c3-9e7a-4010-bb94-9fdf00ad8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fields_v1(example):\n",
    "    text = f\"###context:\\n{example['Context']}\\n\\n###cell_sentences_data:\\n{example['cell_sentences_data']}\\n\\n###Question:\\n{example['Question']}\\n\\n###Answer:\\n{example['Answer']}\"\n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "test_set = test_set.map(combine_fields_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vandijklab/C2S-Pythia-410m-diverse-single-and-multi-cell-tasks\"\n",
    "adapter_id = 'sft_output/checkpoint-2800'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.add_tokens([\"<|Question|>\", \"<|Answer|>\"]) \n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.padding_side == 'right':\n",
    "    tokenizer.padding_side = 'left'\n",
    "model = PeftModel.from_pretrained(model, adapter_id)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9305a-5571-4832-a027-7f47094c66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240af73a-5735-468b-a07f-7f5ff93c14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe7e5de-3ced-4c31-8bff-d31a75178c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_set))):\n\u001b[1;32m      2\u001b[0m     question \u001b[39m=\u001b[39m test_set[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][i]\n\u001b[1;32m      3\u001b[0m     \u001b[39m# Store original question by extracting text between <|Question|> and <|Answer|>\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_set))):\n",
    "    question = test_set['text'][i]\n",
    "    # Store original question by extracting text between <|Question|> and <|Answer|>\n",
    "    original_question = question.split('###Question:')[-1].split('###Answer:')[0].strip()\n",
    "    # Generate 5 answers for the same question\n",
    "    cans = generator(question, max_new_tokens=512, num_return_sequences=5, do_sample=True)\n",
    "    # Extract just the answer portion after <|Answer|> tag\n",
    "    answers = []\n",
    "    for c in cans:\n",
    "        text = c['generated_text']\n",
    "        answer = text.split('###Answer:')[-1].strip()\n",
    "        answers.append({\n",
    "            'question': original_question,\n",
    "            'answer': answer\n",
    "        })\n",
    "    all_cans.append(answers)\n",
    "    \n",
    "    # Save all_cans after each iteration\n",
    "    with open('all_cans.json', 'w') as f:\n",
    "        json.dump(all_cans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea219293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sz568/.conda/envs/C2S_rl310/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Get keywords and ground truth answers\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m keywords_list \u001b[39m=\u001b[39m test_set[\u001b[39m'\u001b[39m\u001b[39mKeyword\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m ground_truth_answers \u001b[39m=\u001b[39m test_set[\u001b[39m'\u001b[39m\u001b[39mAnswer\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[39m# Calculate BERT scores against ground truth\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Get keywords and ground truth answers\n",
    "keywords_list = test_set['Keyword']\n",
    "ground_truth_answers = test_set['Answer']\n",
    "\n",
    "# Calculate BERT scores against ground truth\n",
    "final_scores = []\n",
    "dpo_pairs = []\n",
    "\n",
    "for i in range(len(all_cans)):\n",
    "    answers = all_cans[i]\n",
    "    # Remove brackets from keywords string and split\n",
    "    keywords = keywords_list[i].strip('[]').split(',')\n",
    "    keywords = [k.strip() for k in keywords]\n",
    "    question = test_set['text'][i]\n",
    "    ground_truth = ground_truth_answers[i]\n",
    "    \n",
    "    # Calculate scores for each answer\n",
    "    scores = []\n",
    "    for ans in answers:\n",
    "        P, R, F1 = score([ans], [ground_truth], lang='en', verbose=False)\n",
    "        bert_score = F1.item()\n",
    "        \n",
    "        # Calculate keyword presence ratio\n",
    "        ans_lower = ans.lower()\n",
    "        present_keywords = sum(1 for kw in keywords if kw.lower() in ans_lower)\n",
    "        keyword_ratio = present_keywords / len(keywords)\n",
    "        if keyword_ratio == 0:\n",
    "            keyword_ratio = 0.01\n",
    "        # Adjust bert_score based on keyword presence ratio\n",
    "        adjusted_score = bert_score * keyword_ratio\n",
    "        \n",
    "        scores.append({\n",
    "            'answer': ans,\n",
    "            'score': adjusted_score\n",
    "        })\n",
    "    \n",
    "    # Sort answers by adjusted score\n",
    "    scores.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Create DPO pairs for all possible combinations of 2 answers\n",
    "    # where the higher scored answer is chosen and lower scored is rejected\n",
    "    for j in range(len(answers)):\n",
    "        for k in range(j+1, len(answers)):\n",
    "            if scores[j]['score'] > scores[k]['score']:\n",
    "                chosen = scores[j]['answer']\n",
    "                rejected = scores[k]['answer']\n",
    "                chosen_score = scores[j]['score']\n",
    "                rejected_score = scores[k]['score']\n",
    "            else:\n",
    "                chosen = scores[k]['answer']\n",
    "                rejected = scores[j]['answer']\n",
    "                chosen_score = scores[k]['score']\n",
    "                rejected_score = scores[j]['score']\n",
    "                \n",
    "            dpo_pair = {\n",
    "                'question': question,\n",
    "                'chosen': chosen,\n",
    "                'rejected': rejected,\n",
    "                'ground_truth': ground_truth,\n",
    "                'chosen_score': float(chosen_score),\n",
    "                'rejected_score': float(rejected_score),\n",
    "                'keywords': keywords\n",
    "            }\n",
    "            dpo_pairs.append(dpo_pair)\n",
    "    \n",
    "    # Calculate average score for this question\n",
    "    avg_score = np.mean([s['score'] for s in scores])\n",
    "    final_scores.append(avg_score)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "final_scores = np.array(final_scores)\n",
    "\n",
    "# Save DPO dataset as JSON file\n",
    "with open('dpo_dataset_with_score.json', 'w') as f:\n",
    "    json.dump(dpo_pairs, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16088f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sz568/.conda/envs/C2S_rl310/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Extracting prompt from train dataset: 100%|██████████| 750/750 [00:00<00:00, 9842.98 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 750/750 [00:00<00:00, 1845.20 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 750/750 [00:01<00:00, 491.39 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharryzhang957\u001b[0m (\u001b[33mresearch_harry\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/radev/scratch/dijk/sz568/C2S_RL/wandb/run-20241209_131743-4skhfnop</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/research_harry/huggingface/runs/4skhfnop' target=\"_blank\">dpo_output</a></strong> to <a href='https://wandb.ai/research_harry/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/research_harry/huggingface' target=\"_blank\">https://wandb.ai/research_harry/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/research_harry/huggingface/runs/4skhfnop' target=\"_blank\">https://wandb.ai/research_harry/huggingface/runs/4skhfnop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 03:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>52.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>10.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.922600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.657200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "# Load and prepare DPO dataset\n",
    "with open('dpo_dataset_with_scorev2.json', 'r') as f:\n",
    "    dpo_data = json.load(f)\n",
    "train_dataset = Dataset.from_list(dpo_data)\n",
    "# Load model and tokenizer\n",
    "model_name = \"vandijklab/C2S-Pythia-410m-diverse-single-and-multi-cell-tasks\"\n",
    "adapter_id = 'sft_output/checkpoint-2800'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Remove model_init_kwargs from the config\n",
    "dpo_config = DPOConfig(\n",
    "    # model_init_kwargs={},  # Remove this line\n",
    "    output_dir=\"dpo_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,  # Use the Dataset object, not the list\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "# Proceed with training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"dpo_final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc0475f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_set.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_set.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m test_set \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(test_data)\n",
      "File \u001b[0;32m/home/sz568/.conda/envs/C2S_rl310/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_set.json'"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "with open('test_set.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "test_set = Dataset.from_list(test_data)\n",
    "\n",
    "# Load the trained DPO model\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\"dpo_final_model\")\n",
    "dpo_model.eval()  # Set to evaluation mode\n",
    "\n",
    "all_cans = []  # Store all generated answers\n",
    "\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    question = test_set[i]['text']\n",
    "    \n",
    "    # Prepare input for model\n",
    "    inputs = tokenizer(question + \" ###Answer:\", return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        outputs = dpo_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and clean up the generated answer\n",
    "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer from after ###Answer: token\n",
    "    if \"###Answer:\" in generated_answer:\n",
    "        answer = generated_answer.split(\"###Answer:\")[1].strip()\n",
    "    else:\n",
    "        answer = generated_answer.strip()\n",
    "        \n",
    "    all_cans.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a9589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Depose！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8ec747-9a0e-4a0a-985c-0de8ba150760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(all_cans, open('answer_generated_1124.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283feda9-fb01-460d-a880-5ac4f82bbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_analysis(refs, cans):\n",
    "    \"\"\"\n",
    "    Analyze the chain of thought in the answers by breaking them down into key components\n",
    "    and comparing with reference text.\n",
    "    \"\"\"\n",
    "    analysis_results = []\n",
    "    \n",
    "    for ref, can in zip(refs, cans):\n",
    "        # Split candidate answer into reasoning steps (based on sentences/paragraphs)\n",
    "        steps = [s.strip() for s in can.split('\\n\\n') if s.strip()]\n",
    "        \n",
    "        # Analyze each reasoning step\n",
    "        step_analysis = []\n",
    "        for i, step in enumerate(steps):\n",
    "            step_info = {\n",
    "                'step_number': i+1,\n",
    "                'content': step,\n",
    "                'type': 'initial_claim' if i==0 else 'supporting_evidence' if i<len(steps)-1 else 'conclusion'\n",
    "            }\n",
    "            step_analysis.append(step_info)\n",
    "            \n",
    "        # Overall analysis\n",
    "        analysis = {\n",
    "            'reference': ref,\n",
    "            'candidate_answer': can,\n",
    "            'num_reasoning_steps': len(steps),\n",
    "            'reasoning_chain': step_analysis,\n",
    "            'has_conclusion': any(s['type']=='conclusion' for s in step_analysis),\n",
    "            'has_evidence': any(s['type']=='supporting_evidence' for s in step_analysis)\n",
    "        }\n",
    "        analysis_results.append(analysis)\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Analyze the chain of thought\n",
    "cot_analysis = chain_of_thought_analysis(refs, cans)\n",
    "\n",
    "# Print analysis results\n",
    "for i, analysis in enumerate(cot_analysis):\n",
    "    print(f\"\\nAnalysis for Answer {i+1}:\")\n",
    "    print(f\"Number of reasoning steps: {analysis['num_reasoning_steps']}\")\n",
    "    print(\"\\nReasoning chain:\")\n",
    "    for step in analysis['reasoning_chain']:\n",
    "        print(f\"\\nStep {step['step_number']} ({step['type']}):\")\n",
    "        print(step['content'])\n",
    "    print(\"\\nHas conclusion:\", analysis['has_conclusion'])\n",
    "    print(\"Has supporting evidence:\", analysis['has_evidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f8e07-2a45-405c-8376-3a5772f4b24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8533b-6121-416b-8a74-5b4a578484af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3844dc-c8df-436d-8454-4d779561498f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0882e-2066-4581-97e5-294fa3625d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd6ad7-9f32-4b69-bab8-31825bacb7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d66a4-203b-41ca-9962-76cf3072e722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc754a-8314-41ca-a103-0cf8508c6145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0ba71-ffd7-4355-8c3c-816a5f476a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dc4fb-005b-4a50-835a-9d6d9460b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "refs = ['Our preliminary results indicate that SAH leads to an increase in NO-M in CSF. This increase of NO-M significantly correlates with the flow velocities in TCDS measurement suggesting that NO plays an important role in the pathogenesis of cerebral vasospasm.',\n",
    " 'A lack of decline in the AMH level in early pregnancy can be used to identify women with a high probability for preterm birth, especially when MSAFP levels are >1 multiple of the median. Monitoring changes in the AMH level between the first and second trimesters of pregnancy may help identify women who would benefit from interventional therapies such as supplemental progesterone.']\n",
    "cans = [\"Yes, the study found a significant correlation between nitric oxide metabolites in cisternal CSF and cerebral vasospasm in patients with a subarachnoid haemorrhage. The patients who developed clinically symptomatic vasospasm showed significantly higher levels of nitric oxide metabolites in CSF compared to the patients with an uncomplicated follow-up. This suggests that nitric oxide metabolites may play a role in the development of cerebral vasospasm in patients with subarachnoid haemorrhage.\\n\\nThe study's findings are consistent with the idea that nitric oxide (NO) is involved in the pathogenesis of cerebral vasospasm. NO is a key molecule in the regulation of vascular tone, and its metabolites can be measured in cisternal CSF as a marker of NO production. The fact that patients with cerebral vasospasm showed higher levels of NO metabolites in CSF suggests that NO production is increased in these patients, which may contribute to the development of vasospasm.\\n\\nThe study's results also have implications for the treatment of cerebral vasospasm in patients with subarachnoid haemorrhage. The use of nitric oxide donors or other NO-releasing agents may be beneficial in preventing or treating cerebral vasospasm. Additionally, the study's findings suggest that monitoring NO metabolites in cisternal CSF may be a useful tool in identifying patients at risk of developing cerebral vasospasm.\\n\\nOverall, the study provides evidence that nitric oxide metabolites in cisternal CSF are associated with cerebral vasospasm in patients with subarachnoid haemorrhage. Further studies are needed to confirm these findings and to explore the potential therapeutic applications of NO-releasing agents in the treatment of cerebral vasospasm.\",\n",
    " 'Yes, changes in antimüllerian hormone (AMH) levels in early pregnancy are associated with preterm birth. This association was found after adjusting for other markers of fetoplacental health, such as maternal serum α-fetoprotein (MSAFP) and maternal weight change between the first and second trimesters. Specifically, women with a stable or rising AMH level in early pregnancy and an MSAFP >1 multiple of the median were at higher risk for preterm birth.\\n\\nThe study found that second trimester AMH levels were not associated with preterm birth, either independently or after controlling for other markers of fetoplacental health. However, a statistically significant association was found after adjusting for MSAFP and maternal weight change. This suggests that AMH levels may be a useful marker for identifying women at higher risk for preterm birth, particularly when combined with other markers of fetoplacental health.\\n\\nThe study also found that most of the risk for preterm birth was identified in women with an MSAFP >1 multiple of the median and who had a stable or rising AMH level in early pregnancy. This suggests that women with high MSAFP levels and stable or rising AMH levels may be at higher risk for preterm birth, and that these women may benefit from closer monitoring and interventions to reduce the risk of preterm birth.\\n\\nOverall, the study suggests that changes in AMH levels in early pregnancy are associated with preterm birth, and that AMH levels may be a useful marker for identifying women at higher risk for preterm birth. However, further research is needed to confirm these findings and to determine the clinical utility of AMH levels as a marker for preterm birth.']\n",
    "\n",
    "# 计算 BERTScore\n",
    "P, R, F1 = score(cans, refs, lang=\"zh\")\n",
    "\n",
    "print(f\"Precision: {P}\")\n",
    "print(f\"Recall: {R}\") \n",
    "print(f\"F1: {F1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
